{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip list\n",
    "# %pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from video_loader import VideoDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, distance_dim):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.distance_dim = distance_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / num_embeddings, 1.0 / num_embeddings)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Flatten inputs except for the embedding dimension\n",
    "        flat_inputs = inputs.view(-1, self.embedding_dim)\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_inputs ** 2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self.embedding.weight ** 2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_inputs, self.embedding.weight.t()))\n",
    "        \n",
    "        # Reshape distances back to the original shape with depth, height, and width dimensions\n",
    "        distances = distances.view(*inputs.shape[:-1], -1)\n",
    "        \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=-1)\n",
    "        encodings = F.one_hot(encoding_indices, self.num_embeddings).float()\n",
    "        \n",
    "        # Use encodings to gather embeddings and compute loss\n",
    "        quantized = torch.matmul(encodings, self.embedding.weight).view(inputs.shape)\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Preserve gradients through the quantization\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        return loss, quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channels, embedding_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.quantizer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost, distance_dim=2)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedding_dim, hidden_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(hidden_channels, input_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()  # Assuming input is normalized to [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.permute(0, 2, 3, 1).contiguous()\n",
    "        loss, quantized = self.quantizer(z)\n",
    "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "        recon_x = self.decoder(quantized)\n",
    "        return recon_x, loss\n",
    "\n",
    "# Example model initialization\n",
    "# vqvae = VQVAE(input_channels=3, hidden_channels=128, num_embeddings=512, embedding_dim=64, commitment_cost=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE3D(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VQVAE3D, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(input_channels, hidden_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(hidden_channels, embedding_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.quantizer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost, distance_dim=3)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(embedding_dim, hidden_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(hidden_channels, input_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()  # Assuming input is normalized to [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.permute(0, 2, 3, 4, 1).contiguous()  # Adjust for 3D\n",
    "        loss, quantized = self.quantizer(z)\n",
    "        quantized = quantized.permute(0, 4, 1, 2, 3).contiguous()  # Adjust back after quantization\n",
    "        recon_x = self.decoder(quantized)\n",
    "        return recon_x, loss, z\n",
    "    \n",
    "    def training_step(self, x, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        recon_x, loss, _ = self(x) # Get loss and compute gradients to update the model\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "# Example model initialization\n",
    "# vqvae3d = VQVAE3D(input_channels=3, hidden_channels=128, num_embeddings=512, embedding_dim=64, commitment_cost=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDecoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, output_length):\n",
    "        super(AudioDecoder, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_length = output_length\n",
    "\n",
    "        # Define the audio decoder architecture\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(65536, 512),  # Adjusted from 64 to 65536 to match the flattened input\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, self.output_length),\n",
    "            nn.Tanh()  # Assuming the output audio is normalized between -1 and 1\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, encoded_video):\n",
    "        # encoded_video shape: (batch_size, num_embeddings, embedding_dim)\n",
    "        # Flatten the encoded video to match the decoder input\n",
    "        batch_size = encoded_video.shape[0]\n",
    "        encoded_video_flat = encoded_video.view(batch_size, -1)\n",
    "        \n",
    "        # Decode to audio waveform\n",
    "        audio_output = self.decoder(encoded_video_flat)\n",
    "        return audio_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE3DWithAudio(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, num_embeddings, embedding_dim, commitment_cost, audio_output_length):\n",
    "        super(VQVAE3DWithAudio, self).__init__()\n",
    "        # VQ-VAE components\n",
    "        self.vqvae3d = VQVAE3D(input_channels, hidden_channels, num_embeddings, embedding_dim, commitment_cost)\n",
    "        \n",
    "        # Audio Decoder component\n",
    "        self.audio_decoder = AudioDecoder(num_embeddings, embedding_dim, audio_output_length)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        recon_x, loss, z = self.vqvae3d(x)\n",
    "        \n",
    "        # For simplicity, let's use the quantized output directly from the encoder as input to the audio decoder\n",
    "        # A more sophisticated approach might involve selecting or processing specific parts of the encoding\n",
    "        _, quantized = self.vqvae3d.quantizer(z)\n",
    "        \n",
    "        # Generate audio from the quantized video encoding\n",
    "        audio_output = self.audio_decoder(quantized)\n",
    "        \n",
    "        return recon_x, audio_output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the enhanced VQ-VAE model with audio decoding capability\n",
    "# vqvae3d_with_audio = VQVAE3DWithAudio(\n",
    "#     input_channels=3, \n",
    "#     hidden_channels=128, \n",
    "#     num_embeddings=512, \n",
    "#     embedding_dim=64, \n",
    "#     commitment_cost=0.25, \n",
    "#     audio_output_length=48000  # For 1 second of audio at 48kHz\n",
    "# )\n",
    "\n",
    "# # Example forward pass with a dummy video input\n",
    "# # Assuming the input video is a 5D tensor: (batch_size, channels, depth, height, width)\n",
    "# # For example, a batch of 4 videos, each with 3 color channels, 16 frames, and 64x64 resolution\n",
    "# dummy_video_input = torch.randn(4, 3, 16, 64, 64)\n",
    "# reconstructed_video, generated_audio, loss = vqvae3d_with_audio(dummy_video_input)\n",
    "\n",
    "# # Here, `reconstructed_video` is the reconstructed video output,\n",
    "# # `generated_audio` is the audio waveform generated from the video,\n",
    "# # and `loss` is the loss from the VQ-VAE quantization process.\n",
    "\n",
    "# generated_audio_int16 = (generated_audio * 32767).short()\n",
    "\n",
    "# generated_audio_int16 = generated_audio_int16.unsqueeze(0)\n",
    "# generated_audio_int16 = generated_audio_int16.squeeze()\n",
    "\n",
    "# # Save the audio directly as a PyTorch tensor without converting to a numpy array\n",
    "# torchaudio.save(f\"generated_audio{len(glob.glob('*'))}.wav\", generated_audio_int16, 48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vqvae3d(model, data_loader, epochs=10, lr=2e-4, accumulation_steps=2, max_norm=1, plot_save_path=None, model_save_path='vqvae3d_model.pth'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_history = []  # Store loss over epochs\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()  # Reset gradients; do it once at the start\n",
    "        \n",
    "        for batch_idx, data in enumerate(data_loader):\n",
    "            data = data.to(device)\n",
    "            recon_x, loss, _ = model(data)\n",
    "            loss = loss / accumulation_steps  # Normalize our loss (if averaged)\n",
    "            loss.backward()  # Accumulate gradients\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:  # Perform optimization every 'accumulation_steps'\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Print the current iteration, overwrite the previous text\n",
    "            print(f'\\rEpoch {epoch}, Iteration {batch_idx + 1}/{len(data_loader)}, Loss: {total_loss}', end='', flush=True)\n",
    "        \n",
    "        epoch_loss = total_loss / len(data_loader)\n",
    "        loss_history.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n",
    "        \n",
    "    if not os.path.isdir(os.path.dirname(model_save_path)):\n",
    "        os.makedirs(os.path.dirname(model_save_path))\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "        \n",
    "    plt.plot(range(1, epochs + 1), loss_history, marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    if plot_save_path:\n",
    "        plt.savefig(plot_save_path)  # Save the plot if a path is provided\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = 256, 144\n",
    "videos_dir = \"/data/ai_club/team_13_2023-24/videos/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 shape: torch.Size([1, 300, 256, 144])\n"
     ]
    }
   ],
   "source": [
    "# Original ImageNet stats for RGB images\n",
    "mean_rgb = [0.485, 0.456, 0.406]\n",
    "std_rgb = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Approximate stats for grayscale\n",
    "mean_gray = sum(mean_rgb) / len(mean_rgb)\n",
    "std_gray = sum(std_rgb) / len(std_rgb)\n",
    "\n",
    "# Define transformations\n",
    "transformations = Compose([\n",
    "    Resize((width, height)),  # Resize to a common size\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean_gray, std=std_gray),  # Imagenet stats\n",
    "])\n",
    "\n",
    "# Create VideoDataset\n",
    "video_dataset = VideoDataset(videos_dir, transform=transformations)\n",
    "\n",
    "# Check dataset\n",
    "print(f\"Sample 0 shape: {video_dataset[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 2/200, Loss: 0.031908092088997364"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2e-4\u001b[39m\n\u001b[1;32m     13\u001b[0m accumulation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrain_vqvae3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvqvae3d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./loss.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mtrain_vqvae3d\u001b[0;34m(model, data_loader, epochs, lr, accumulation_steps, max_norm, plot_save_path, model_save_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 22\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Print the current iteration, overwrite the previous text\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Device training on\n",
    "device = \"cuda\"\n",
    "\n",
    "# Create the model\n",
    "vqvae3d = VQVAE3D(input_channels=1, hidden_channels=128, num_embeddings=512, embedding_dim=64, commitment_cost=0.25).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "data_loader = DataLoader(video_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 32\n",
    "learning_rate = 2e-4\n",
    "accumulation_steps = 2\n",
    "\n",
    "train_vqvae3d(vqvae3d, data_loader, epochs=epochs, lr=learning_rate, accumulation_steps=accumulation_steps, plot_save_path=\"./loss.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
